# Project Directory Structure:
.
├── .gitattributes
├── .gptree_config
├── .python-version
├── .ruff_cache/
├── README.md
├── gptree_output.txt
├── pyproject.toml
├── src/
│   └── rl_david_silver_2015/
│       ├── __init__.py
│       └── mdp/
│           ├── abstract_mdpf.py
│           ├── common.py
│           ├── sampler.py
│           ├── sampler_generator.py
│           ├── tabular_mdpf.py
│           ├── test.py
│           └── test2.py
└── uv.lock

# BEGIN FILE CONTENTS

# File: src/rl_david_silver_2015/mdp/test2.py

from rl_david_silver_2015.mdp.sampler import jit_sample_mdp_n_steps
from rl_david_silver_2015.mdp.tabular_mdpf import (
    SAMPLE_TABULAR_STARTING_STATE,
    SAMPLE_TABULAR_MDP,
    SAMPLE_TABULAR_POLICY,
    TabularMDPFramework,
)


r = jit_sample_mdp_n_steps(
    SAMPLE_TABULAR_MDP,
    SAMPLE_TABULAR_POLICY,
    TabularMDPFramework,
    SAMPLE_TABULAR_STARTING_STATE,
)

print(r)


# END FILE CONTENTS


# File: src/rl_david_silver_2015/mdp/abstract_mdpf.py

from abc import ABC, abstractmethod
from typing import Callable, Generic, TypeVar

from rl_david_silver_2015.mdp.common import (
    BatchedActionType,
    BatchedStateType,
    BatchedTerminal,
    MDPSample,
    RandomKey,
)

MDPType = TypeVar("MDPType")
PolicyType = TypeVar("PolicyType")
TerminalPredicateType = TypeVar(
    "TerminalEvaluatorType", bound=Callable[[BatchedStateType], BatchedTerminal]
)


class AbstractMDPFramework(
    ABC,
    Generic[
        MDPType, PolicyType, BatchedStateType, BatchedActionType, TerminalPredicateType
    ],
):
    @staticmethod
    @abstractmethod
    def get_terminal_state_predicate(mdp: MDPType) -> TerminalPredicateType:
        """
        Get the terminal states of the MDP.
        """
        raise NotImplementedError(
            "This method should be implemented to return terminal states."
        )

    @staticmethod
    @abstractmethod
    def sample_mdp(
        mdp: MDPType,
        states: BatchedStateType,
        actions: BatchedActionType,
        random_key: RandomKey,
    ) -> MDPSample[BatchedStateType]:
        """
        Sample the next state and reward from the MDP given the current state and action.
        """
        raise NotImplementedError(
            "This method should be implemented to sample from the MDP."
        )

    @staticmethod
    @abstractmethod
    def sample_policy(
        policy: PolicyType, states: BatchedStateType, random_key: RandomKey
    ) -> BatchedActionType:
        """
        Sample an action from the policy given the current state.
        """
        raise NotImplementedError(
            "This method should be implemented to sample from the policy."
        )


# END FILE CONTENTS


# File: src/rl_david_silver_2015/mdp/common.py

from typing import Callable, Generic, NamedTuple, TypeVar

from jaxtyping import Array, Float, Bool
import jax

MDPType = TypeVar("MDPType")
PolicyType = TypeVar("PolicyType")
BatchedStateType = TypeVar("BatchedStateType")
BatchedActionType = TypeVar("BatchedActionType")
BatchedReward = Float[Array, "b"]  # Batched rewards
BatchedTerminal = Bool[Array, "b"]  # Batched terminal status
RandomKey = Array  # Typically a JAX PRNG key


class MDPSample(NamedTuple, Generic[BatchedStateType]):
    r_tp1: BatchedReward  # rewards for next states
    s_tp1: BatchedStateType  # next states


MDPSamplingFunction = Callable[
    [MDPType, BatchedStateType, BatchedActionType, RandomKey],
    MDPSample[BatchedStateType],
]
PolicySamplingFunction = Callable[
    [PolicyType, BatchedStateType, RandomKey], BatchedActionType
]


class BatchedAgentUpdate(NamedTuple, Generic[BatchedActionType]):
    a_t: BatchedActionType


class BatchedEnvironmentUpdate(NamedTuple, Generic[BatchedStateType]):
    r_tp1: BatchedReward
    s_tp1: BatchedStateType
    terminal: BatchedTerminal


BatchTerminalStateEvaluator = Callable[[BatchedStateType], BatchedTerminal]

DEFAULT_RANDOM_KEY = jax.random.PRNGKey(0)


# END FILE CONTENTS


# File: src/rl_david_silver_2015/mdp/tabular_mdpf.py

from dataclasses import dataclass
from typing import NamedTuple
from rl_david_silver_2015.mdp.abstract_mdpf import AbstractMDPFramework
from jaxtyping import Array, Float, Int, Bool
import jax.numpy as jnp
import jax

from rl_david_silver_2015.mdp.common import MDPSample, RandomKey


class TabularMDP(NamedTuple):
    """
    Information to represent a Markov Decision Process (MDP).
    S, A are implicitly defined by the indices for states and actions.
    """

    P: Float[Array, "n_states n_actions n_states"]  # transition function
    R: Float[Array, "n_states n_actions"]  # reward function
    gamma: Float[Array, ""]  # discount factor

    @property
    def n_states(self) -> int:
        return self.P.shape[0]

    @property
    def n_actions(self) -> int:
        return self.P.shape[1]


TabularPolicy = Float[
    Array, "n_states n_actions"
]  # Policy represented as a matrix of action probabilities

TabularBatchedState = Int[Array, "b"]  # Batched states
TabularBatchedAction = Int[Array, "b"]  # Batched actions


@dataclass(frozen=True)
class TabularTerminalPredicate:
    terminal_states: Int[Array, "n_terminal_states"]

    def __call__(self, states: TabularBatchedState) -> Bool[Array, "b"]:
        """
        Evaluate if the given states are terminal.
        """
        return jnp.isin(states, self.terminal_states)


class TabularMDPFramework(
    AbstractMDPFramework[
        TabularMDP,
        TabularPolicy,
        TabularBatchedState,
        TabularBatchedAction,
        TabularTerminalPredicate,
    ]
):

    @staticmethod
    def get_terminal_state_predicate(mdp: TabularMDP) -> TabularBatchedState:
        """
        Get the terminal states of the MDP.
        """
        eye = jnp.eye(mdp.P.shape[0])  # Identity matrix of shape (n_states, n_states)
        match_self = jnp.all(mdp.P == eye[:, None, :], axis=-1)
        terminal_mask = jnp.all(match_self, axis=-1)  # shape: (n_states,)
        return TabularTerminalPredicate(
            jnp.nonzero(terminal_mask, size=mdp.n_states)[0]
        )

    @staticmethod
    def sample_mdp(
        mdp: TabularMDP,
        states: TabularBatchedState,
        actions: TabularBatchedAction,
        random_key: RandomKey,
    ) -> MDPSample[TabularBatchedState]:
        """
        Vectorized transition sampling for batches of states and actions, using logits,
        while ensuring transitions with zero probability are never sampled.
        """

        batch_size = states.shape[0]
        keys = jax.random.split(random_key, batch_size)

        # Extract transition probabilities
        transition_probs = mdp.P[states, actions]  # shape: (b, n_states)
        transition_logits = jnp.where(
            transition_probs > 0, jnp.log(transition_probs), -jnp.inf
        )  # shape: (b, n_states)

        # Sample next states using categorical distribution over logits
        next_states = jax.vmap(jax.random.categorical)(keys, transition_logits)

        # Get rewards and terminal status
        rewards = mdp.R[states, actions]  # shape: (b,)

        return MDPSample(r_tp1=rewards, s_tp1=next_states)

    @staticmethod
    def sample_policy(
        policy: TabularPolicy, states: TabularBatchedState, random_key: RandomKey
    ) -> TabularBatchedAction:
        """
        Sample an action from the policy given the current state.
        """
        batch_size = states.shape[0]
        keys = jax.random.split(random_key, batch_size)

        # Extract action probabilities for the given states
        action_probs = policy[states]  # shape: (b, n_actions)

        # Turn action probabilities into logits
        action_logits = jnp.where(action_probs > 0, jnp.log(action_probs), -jnp.inf)

        # Sample actions using categorical distribution over logits
        actions = jax.vmap(jax.random.categorical)(keys, action_logits)
        return actions


"""
Sample Tabular MDP with 3 states and 2 actions (starting state, left or right, left state, right state)

State 0: Starting state
State 1: Left state
State 2: Right state

Action 1: Move left
Action 2: Move right

Terminal states: State 1 and State 2
"""
SAMPLE_TABULAR_MDP = TabularMDP(
    P=jnp.array(
        [
            [[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],  # State 0 transitions
            [[0.0, 1.0, 0.0], [0.0, 1.0, 0.0]],  # State 1 transitions
            [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]],  # State 2 transitions
        ]
    ),
    R=jnp.array(
        [
            [0.0, 0.0],  # Rewards for actions in state 0
            [-1.0, -1.0],  # Rewards for actions in state 1
            [-1.0, -1.0],  # Rewards for actions in state 2
        ]
    ),
    gamma=jnp.array(0.9),  # Discount factor
)

SAMPLE_TABULAR_STARTING_STATE = jnp.array([0, 0])  # Starting state for the sample

SAMPLE_TABULAR_POLICY = jnp.array(
    [
        [0.5, 0.5],  # Policy for state 0: equal probability for left and right
        [0.5, 0.5],  # Policy for state 1: equal probability for left and right
        [0.5, 0.5],  # Policy for state 2: equal probability for left and right
    ]
)


# END FILE CONTENTS


# File: src/rl_david_silver_2015/mdp/sampler.py

from typing import Callable, Tuple
from rl_david_silver_2015.mdp.abstract_mdpf import AbstractMDPFramework, MDPType, PolicyType
from rl_david_silver_2015.mdp.common import DEFAULT_RANDOM_KEY, BatchedActionType, BatchedReward, BatchedStateType, BatchedTerminal, MDPSample, RandomKey
import jax
from jax import lax

@jax.jit
def jit_sample_mdp_n_steps(
    mdp: MDPType,
    policy: PolicyType,
    mdp_framework: type[AbstractMDPFramework[
        MDPType, PolicyType, BatchedStateType, BatchedActionType,
        Callable[[BatchedStateType], BatchedTerminal]
    ]],
    initial_state: BatchedStateType,
    n_steps: int = 1000,
    random_key: RandomKey = DEFAULT_RANDOM_KEY,
) -> Tuple[
    BatchedStateType,               # final state
    BatchedActionType,             # [n_steps, batch_size]
    BatchedReward,                 # [n_steps, batch_size]
    BatchedStateType,              # [n_steps, batch_size]
    BatchedTerminal                # [n_steps, batch_size]
]:
    """
    Fully JIT-compatible MDP sampler that runs for a fixed number of steps.
    Tracks actions, rewards, states, terminals at each step.
    """

    batch_size = initial_state.shape[0]
    terminal_predicate = mdp_framework.get_terminal_state_predicate(mdp)

    def scan_step(
        carry: Tuple[BatchedStateType, RandomKey],
        _,
    ) -> Tuple[
        Tuple[BatchedStateType, RandomKey],
        Tuple[BatchedActionType, BatchedReward, BatchedStateType, BatchedTerminal],
    ]:
        s_t, key = carry
        key, action_key, state_key = jax.random.split(key, 3)

        a_t = mdp_framework.sample_policy(policy, s_t, action_key)
        sample: MDPSample[BatchedStateType] = mdp_framework.sample_mdp(mdp, s_t, a_t, state_key)
        terminal = terminal_predicate(sample.s_tp1)

        return (sample.s_tp1, key), (a_t, sample.r_tp1, sample.s_tp1, terminal)

    (final_state, _), (actions, rewards, states, terminals) = lax.scan(
        scan_step,
        (initial_state, random_key),
        xs=None,
        length=n_steps,
    )

    return final_state, actions, rewards, states, terminals

# END FILE CONTENTS


# File: src/rl_david_silver_2015/mdp/sampler_generator.py

from typing import Callable, Generic, Iterator, NamedTuple, Type, TypeVar
from jaxtyping import Array, Int, Float, Bool
import jax
import jax.numpy as jnp
from rl_david_silver_2015.mdp.common import BatchedTerminal, RandomKey
from rl_david_silver_2015.mdp.abstract_mdpf import (
    AbstractMDPFramework,
    BatchedStateType,
    MDPType,
    PolicyType,
)
from rl_david_silver_2015.mdp.common import (
    DEFAULT_RANDOM_KEY,
    BatchedActionType,
    BatchedReward,
    MDPSample,
)

MDPFrameworkType = TypeVar("MDPFrameworkType", bound="AbstractMDPFramework")


class BatchedAgentUpdate(NamedTuple, Generic[BatchedActionType]):
    seq_idx: Int[Array, "..."]
    a_t: BatchedActionType


class BatchedEnvironmentUpdate(NamedTuple, Generic[BatchedStateType]):
    seq_idx: Int[Array, "..."]
    r_tp1: BatchedReward
    s_tp1: BatchedStateType
    terminal: BatchedTerminal


def sample_mdp_batched_generator(
    mdp: MDPType,
    policy: PolicyType,
    mdp_framework: type[
        AbstractMDPFramework[
            MDPType,
            PolicyType,
            BatchedStateType,
            BatchedActionType,  # now concrete, not “left open”
            Callable[[BatchedStateType], BatchedTerminal],
        ]
    ],
    s_1s: BatchedStateType,
    max_n: int = 1000,
    random_key: Array = DEFAULT_RANDOM_KEY,
) -> Iterator[
    BatchedAgentUpdate[BatchedActionType] | BatchedEnvironmentUpdate[BatchedStateType]
]:
    """
    Allows sampling of a batch of episodes from the MDP.
    Also allows for early termination of subset episodes if they reach terminal states.
    """
    terminal_predicate = mdp_framework.get_terminal_state_predicate(mdp)

    # Future work: Turn this into a class that allows dynamically adding new episodes.
    # Future work: Turn this into a JIT-able function? - Or not. This function is meant to be for fast BUT INTERACTIVE use cases where we visualize the sampling process.
    assert isinstance(
        s_1s, jnp.ndarray
    ), "This sampler assumes that actions and samples are represented as JAX arrays."
    s_t = s_1s  # current state
    seq_idx = jnp.arange(s_1s.shape[0])  # sequence for each episode being run in batch
    seq_idx_mask = jnp.ones_like(seq_idx, dtype=bool)  # mask for the sequence index
    t = 1

    while bool(jnp.any(seq_idx_mask)) and t < max_n:

        random_key, action_key, state_key = jax.random.split(random_key, 3)

        assert isinstance(random_key, RandomKey)
        assert isinstance(action_key, RandomKey)
        assert isinstance(state_key, RandomKey)

        seq_subset = seq_idx[seq_idx_mask]
        s_t_masked: BatchedStateType = s_t[seq_idx_mask]

        # Sample actions for the current states
        a_t = mdp_framework.sample_policy(policy, s_t_masked, action_key)
        yield BatchedAgentUpdate(seq_subset, a_t)

        # Sample next states and rewards
        mdp_return = mdp_framework.sample_mdp(mdp, s_t_masked, a_t, state_key)
        is_terminal = terminal_predicate(mdp_return.s_tp1)
        yield BatchedEnvironmentUpdate(
            seq_subset, mdp_return.r_tp1, mdp_return.s_tp1, is_terminal
        )

        s_t = s_t.at[seq_subset].set(mdp_return.s_tp1)
        new_terminal_idx = seq_subset[is_terminal]

        # Update the sequence index mask by masking new terminal states
        seq_idx_mask = seq_idx_mask.at[new_terminal_idx].set(False)
        t += 1


# END FILE CONTENTS
